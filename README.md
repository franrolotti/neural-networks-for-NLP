# Neural Networks for NLP

This repository contains a collection of Jupyter notebooks exploring **neural network architectures for Natural Language Processing (NLP)**.  
It was developed as part of coursework and self-study to practice fundamental and advanced concepts in deep learning for text.

---

## ðŸ“‚ Repository Structure

- `word_embeddings.ipynb` â€” Introduction to word embeddings (Word2Vec, GloVe, embedding matrix analysis).
- `lstm.ipynb` â€” Language modeling with LSTM networks.
- `gru.ipynb` â€” Sequence modeling using GRU networks.
- `language_modelling_lstm.ipynb` â€” Hands-on example of language modeling with LSTM.
- `bidirectional_rnn.ipynb` â€” Implementation of bidirectional RNNs and analysis of embedding matrix evolution.
- `data/` â€” Dataset(s) and preprocessed files used for experiments.
- `README.md` â€” Project documentation (this file).

---

## ðŸš€ Features & Goals

- Build and train RNN-based models for sequence and language modeling tasks.
- Compare different architectures: **RNN, LSTM, GRU, Bidirectional RNN**.
- Explore **word embeddings** and their evolution during training.
- Provide **educational examples** for learners interested in deep learning and NLP.
